{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning - Fifa 2018\n",
    "\n",
    "The following example, explains in simple words, a machine learning tasks.\n",
    "\n",
    "- Database that is going to be used: Fifa Database 2018\n",
    "- Algorithm that is going to be tested: Decision Tree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Import databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Import three databases\n",
    "df1 = pd.read_csv(\"RESULTS.csv\")\n",
    "df2 = pd.read_csv(\"TEST.csv\")\n",
    "df3 = pd.read_csv(\"TRAIN.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above database are:\n",
    "\n",
    "- df3 --> Train database. It contains all the data features from the FIFA database.\n",
    "- df2 --> Test database. It contains a test set sample, with empty the column that we want to predict (Feature: Overall).\n",
    "- df1 --> Results database. It contains the test set sample, with filled the column that we want to predict (Feature: Overall). The filled one is the accurate one, the one that we actually want to predict in the df2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the NaN values from the TRAIN database\n",
    "\n",
    "df3 = df3.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason that we drop the empty cell is because the algorithm that we are going to apply later on, cannot accept NaN values. There are different wayss to deal with missing values. For the particular example it was decided to remove those rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Select y and X\n",
    "\n",
    "- **y** is the feature that we want to predict. In this case is Overall performance of the footballers\n",
    "- **X** are the features that we want to use, to fit a model. It is important to have all the features in numbers. Python, recognizes ONLY numeric features for training algorithms.\n",
    "\n",
    "What is a model? A model is something that we can use, in order to make a prediction\n",
    "What do you mean \"fit\"? It means that we take into consideration the features and assumes how did they provide the respective \"Overall\" score for a footballer (per row). This consideration builds the model, and allows to generalize our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define y\n",
    "y = df3[[\"Overall\"]]\n",
    "\n",
    "# Define X\n",
    "features = ['Crossing',\n",
    "       'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling',\n",
    "       'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration',\n",
    "       'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower',\n",
    "       'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression',\n",
    "       'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure',\n",
    "       'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling',\n",
    "       'GKKicking', 'GKPositioning', 'GKReflexes']\n",
    "\n",
    "X = df3[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Train the algorithm (Decision Tree)\n",
    "\n",
    "For this particular experiment, there was no particular reason for selecting the Decision Tree algorithm. This experiment conducted purely to provide a view of what the algorithm does"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the required algorithm\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "\n",
    "# Select the algorithm\n",
    "Trainer = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Train the model\n",
    "Trainer.fit(X,y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub-Part 3.1: Calculating MAE\n",
    "\n",
    "Calculating MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "val_mae = mean_absolute_error(df3[\"Overall\"], y)\n",
    "val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Make a prediction\n",
    "\n",
    "The following script aims to predict TEST database, Overall empty cells based on the model that we trained earlier (Trainer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this to happen, we have to take into considerration that the TEST database is using different X features.\n",
    "X1 = df2[features]\n",
    "X1 = X1.groupby(X1.columns, axis = 1).transform(lambda x: x.fillna(x.mean()))\n",
    "X1 =X1.fillna(value=62)\n",
    "\n",
    "# And also, we have to apply the model that we fit earlier to the database.\n",
    "predictions = Trainer.predict(X1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Calculating MAE\n",
    "\n",
    "Calculating MAE is one simple way to see the performance of the algorithm. It is the sum of the deduction of the actual value - the predicted one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7602050155592166"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "val_mae = mean_absolute_error(df1[\"Overall\"], predictions)\n",
    "val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above calculation is not bad but there are many ways to reduce the MAE. Some of the ways are mentioned below:\n",
    "\n",
    "1. Dealing differently with the **missing values**.\n",
    "2. Including **additional features** (adding or removing some of them - hint: coefficients might play a vital role here)\n",
    "3. **Tune** the algorithm. Hmmm that sounds interesting, let us stay on that track and see what else can we do here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Tune the algorithm (test 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 2  \t\t Mean Absolute Error:  4\n",
      "Max leaf nodes: 5  \t\t Mean Absolute Error:  3\n",
      "Max leaf nodes: 25  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 50  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 80  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 100  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 120  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 150  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 180  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 200  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 250  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 500  \t\t Mean Absolute Error:  2\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# Select the Leaf Nodes\n",
    "candidate_max_leaf_nodes = [2, 5, 25, 50,80, 100,120,150,180,200, 250, 500]\n",
    "\n",
    "r = df1[\"Overall\"]\n",
    "\n",
    "def get_mae(max_leaf_nodes, X, X1,y,r):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(X, y)\n",
    "    preds_val = model.predict(X1)\n",
    "    mae = mean_absolute_error(r, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n",
    "for max_leaf_nodes in [2, 5, 25, 50,80, 100,120,150,180,200, 250, 500]:\n",
    "    my_mae = get_mae(max_leaf_nodes, X, X1, y, df1[\"Overall\"])\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "\n",
    "# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\n",
    "scores = {leaf_size: get_mae(leaf_size, X, X1, y, df1[\"Overall\"]) for leaf_size in candidate_max_leaf_nodes}\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "print (best_tree_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Tune the algorithm (test 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max leaf nodes: 70  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 71  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 72  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 73  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 74  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 75  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 76  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 77  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 78  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 79  \t\t Mean Absolute Error:  2\n",
      "Max leaf nodes: 80  \t\t Mean Absolute Error:  2\n",
      "74\n"
     ]
    }
   ],
   "source": [
    "# Select the Leaf Nodes\n",
    "candidate_max_leaf_nodes = [70,71,72,73,74,75,76,77,78,79,80]\n",
    "\n",
    "r = df1[\"Overall\"]\n",
    "\n",
    "def get_mae(max_leaf_nodes, X, X1,y,r):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(X, y)\n",
    "    preds_val = model.predict(X1)\n",
    "    mae = mean_absolute_error(r, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n",
    "for max_leaf_nodes in [70,71,72,73,74,75,76,77,78,79,80]:\n",
    "    my_mae = get_mae(max_leaf_nodes, X, X1, y, df1[\"Overall\"])\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "\n",
    "# Store the best value of max_leaf_nodes (it will be either 5, 25, 50, 100, 250 or 500)\n",
    "scores = {leaf_size: get_mae(leaf_size, X, X1, y, df1[\"Overall\"]) for leaf_size in candidate_max_leaf_nodes}\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "print (best_tree_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 8: Fit the model with the best number of  trees this time\n",
    "\n",
    "The most optimal result, is using 74 leaf nodes. This is the number of leaves that we are going to use to optimize our model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=74, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the algorithm\n",
    "final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n",
    "\n",
    "# Fit the final model\n",
    "final_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 9: Make a prediction and check the new MAE\n",
    "\n",
    "As we have created a new model, it is time to see how much was the model benefited after the tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.436140467365015"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = final_model.predict(X1)\n",
    "val_mae = mean_absolute_error(df1[\"Overall\"], predictions)\n",
    "val_mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 10: Train a new algorithm - Random Forest\n",
    "\n",
    "Let us see also if another, more flexible algorithm could perform any better on that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DIMA\\AppData\\Local\\Programs\\Python\\Python37-32\\lib\\site-packages\\ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0259968576484226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#y = y[\"Overall\"]\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1,n_estimators=48)\n",
    "forest_model.fit(X, y)\n",
    "melb_preds = forest_model.predict(X1)\n",
    "print(mean_absolute_error(r, melb_preds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is presented the optimal estimator for the Random Forest, with the best performance of MAE (48)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'75: 2.036 \\n74: 2.033 \\n73: 2.032 \\n70: 2.031 \\n60: 2.030 \\n30: 2.057 \\n45: 2.028 \\n47: 2.029 \\n48: 2.025 \\n49: 2.034 \\n50: 2.035 '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"75: 2.036 \n",
    "74: 2.033 \n",
    "73: 2.032 \n",
    "70: 2.031 \n",
    "60: 2.030 \n",
    "30: 2.057 \n",
    "45: 2.028 \n",
    "47: 2.029 \n",
    "48: 2.025 \n",
    "49: 2.034 \n",
    "50: 2.035 \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
